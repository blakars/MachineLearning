{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klassen & Zweck:\n",
    "* Abstrakte Klasse \"Classifier\" als Basis-Klasse für unterschiedliche Klassifikations-Verfahren/Algorithmen\n",
    "* Klasse \"KNNClassifier\": Naive Umsetzung einer KNN-Klassifikation\n",
    "* Klasse \"FastKNNClassifier\": Effizientere Umsetzung einer KNN-Klassifikation mittels KD-Trees\n",
    "\n",
    "Methoden der Basis-Klasse \"Classifier\":\n",
    "\n",
    "* **_ _ init _ _(self,C)**: Konstruktor der Basis-Klasse, C=Anzahl der unterschiedlichen Klassen, abgeleitete Klassen rufen diesen Konstruktor dann auf\n",
    "* **fit(self,X,T)**: Abstrakte Methode zum \"Traineren\" der Klassifikation. Für die naive KNN-Klassifikation bedeutet \"Traineren\" lediglich \"Abspeichern\", für die KD-Tree Version von KNN erfolgt hier das Erstellen des KDTree\n",
    "* **predict(self,x)**: Abstrakte Methode, die in der Umsetzung dann den eigentlichen Klassifikations-Algorithmus enthält, d.h. die Vorhersage, dass ein gegebener Vektor x zu einer bestimmten Klasse gehört\n",
    "* **crossvalidate(self,S,X,T)**: Methode zur Kreuz-Validierung eines in S Teile aufgeteilten Daten-Sets, benötigt S Trainings-Durchläufe, liefert Generalisierungsfehler \n",
    "\n",
    "\n",
    "für Details siehe auch V1A2_Classifier.html (erstellt mit pydoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Für einen kNN-Klassifikator, bedeutet \"Lernen\" in dem Fall einfach nur, dass er die Zuordnung von Daten-Vektoren (aus Matrix X) zu unterschiedlichen Klassen-Labels (Vektor T) abspeichert. Außerdem ruft die Funktion fit(self,X,T) der \"KNNClassifier\"-Klasse noch die fit-Methode der abstrakten Klasse \"Classifier\" auf, in der geprüft wird ob die Dimensionen von X und T auch (zueinander) passen und die Anzahl unterschiedlicher Klassen C auf Basis des Übergebenen Vektors T aktualisiert wird.\n",
    "- Implementierung s.u.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementierung 2b-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix X=\n",
      " [[1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]\n",
      " [4 5 6]]\n",
      "Class labels T=\n",
      " [0 1 0 1]\n",
      "Test vector x= [2 3 4]\n",
      "Euklidean distances d= [1.7320508075688772, 0.0, 1.7320508075688772, 3.4641016151377544]\n",
      "\n",
      "Classification with the naive KNN-classifier:\n",
      "Test vector is most likely from class  1\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [0.0, 1.0]\n",
      "Indexes of the k= 1  nearest neighbors: idx_knn= [1]\n",
      "\n",
      "Classification with the fast KNN-classifier:\n",
      "Test vector is most likely from class  0\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [0.6666666666666666, 0.3333333333333333]\n",
      "Indexes of the k= 3  nearest neighbors: idx_knn= [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Python Module for Classification Algorithms\n",
    "# Programmgeruest zu Versuch 1, Aufgabe 2\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "from random import randint\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# Base class for classifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for a classifier.\n",
    "    Inherit from this class to implement a concrete classification algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=2): \n",
    "        \"\"\"\n",
    "        Constructor of class Classifier\n",
    "        Should be called by the constructors of derived classes\n",
    "        :param C: Number of different classes\n",
    "        \"\"\"\n",
    "        self.C = C            # set C=number of different classes \n",
    "\n",
    "    def fit(self,X,T):    \n",
    "        \"\"\" \n",
    "        Train classier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        shapeX,shapeT=X.shape,T.shape  # X must be a N x D matrix; T must be a N x 1 matrix; N is number of data vectors; D is dimensionality\n",
    "        assert len(shapeX)==2, \"Classifier.fit(self,X,T): X must be two-dimensional array!\"\n",
    "        assert len(shapeT)==1, \"Classifier.fit(self,X,T): T must be one-dimensional array!\"\n",
    "        assert shapeX[0]==shapeT[0], \"Classifier.fit(self,X,T): Data matrix X and class labels T must have same length!\"\n",
    "        self.C=max(T)+1;       # number of different integer-type class labels (assuming that T(i) is in {0,1,...,C-1})\n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\" \n",
    "        Implementation of classification algorithm, should be overwritten in any derived class\n",
    "        :param x: test data vector\n",
    "        :returns: label of most likely class that test vector x belongs to (and possibly additional information)\n",
    "        \"\"\"\n",
    "        return -1,None,None\n",
    "\n",
    "    def crossvalidate(self,S,X,T):    # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Vector of class labels; T[n] is label of X[n]\n",
    "        :returns pClassError: probability of a classification error (=1-Accuracy)\n",
    "        :returns pConfErrors: confusion matrix, pConfErrors[i,j] is the probability that a vector from true class j will be mis-classified as class i\n",
    "        \"\"\"\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        Xp,Tp=[X[i] for i in perm], [T[i] for i in perm]    # ... to get a random partition of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        C=max(T)+1;                                         # number of different class labels (assuming that t is in {0,1,...,C-1})\n",
    "        nC          = np.zeros(C)                           # initialize class probabilities: nC[i]:=N*pr[xn is of class i]\n",
    "        pConfErrors = np.zeros((C,C))                       # initialize confusion error probabilities pr[class i|class j]\n",
    "        pClassError = 0                                     # initialize probability of a classification error\n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = [Xp[i] for i in idxLearn], [Tp[i] for i in idxLearn]   # learning data for training the classifier\n",
    "            X_test , T_test  = [Xp[i] for i in idxTest] , [Tp[i] for i in idxTest]    # test data \n",
    "            self.fit(np.array(X_learn),np.array(T_learn))                             # train classifier\n",
    "            # (ii) test classifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) classify i-th test vector\n",
    "                t_test = self.predict(X_test[i])[0]             # classify test vector\n",
    "                # (ii.b) check for classification errors\n",
    "                t_true = T_test[i]                              # true class label\n",
    "                nC[t_true]=nC[t_true]+1                         # count occurrences of individual classes\n",
    "                pConfErrors[t_test][t_true]=pConfErrors[t_test][t_true]+1  # count conditional class errors\n",
    "                if(t_test!=t_true): pClassError=pClassError+1              # count total number of errors\n",
    "        pClassError=float(pClassError)/float(N)         # probability of a classification error\n",
    "        for i in range(C): \n",
    "            for j in range(C): \n",
    "                pConfErrors[i,j]=float(pConfErrors[i,j])/float(nC[j])   # finally compute confusion error probabilities\n",
    "        self.pClassError,self.pConfErrors=pClassError,pConfErrors       # store error probabilities as object fields\n",
    "        return pClassError, pConfErrors                 # return error probabilities\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# (Naive) k-nearest-neighbor classifier based on simple look-up-table and exhaustive search\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class KNNClassifier(Classifier):\n",
    "    \"\"\"\n",
    "    (Naive) k-nearest-neighbor classifier based on simple look-up-table and exhaustive search\n",
    "    Derived from base class Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=2,k=1):\n",
    "        \"\"\"\n",
    "        Constructor of the KNN-Classifier\n",
    "        :param C: Number of different classes\n",
    "        :param k: Number of nearest neighbors that classification is based on\n",
    "        \"\"\"\n",
    "        Classifier.__init__(self,C) # call constructor of base class  \n",
    "        self.k = k                  # k is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]      # initially no data is stored\n",
    "\n",
    "    def fit(self,X,T):\n",
    "        \"\"\"\n",
    "        Train classifier; for naive KNN Classifier this just means to store data matrix X and label vector T\n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        Classifier.fit(self,X,T);   # call to base class to check for matrix dimensions etc.\n",
    "        self.X, self.T = X,T        # just store the N x D data matrix and the N x 1 label matrix (N is number and D dimensionality of data vectors) \n",
    "        \n",
    "    def getKNearestNeighbors(self, x, k=None, X=None):\n",
    "        \"\"\"\n",
    "        compute the k nearest neighbors for a query vector x given a data matrix X\n",
    "        :param x: the query vector x\n",
    "        :param X: the N x D data matrix (in each row there is data vector) as a numpy array\n",
    "        :param k: number of nearest-neighbors to be returned\n",
    "        :return: list of k line indexes referring to the k nearest neighbors of x in X\n",
    "        \"\"\"\n",
    "        if(k==None): k=self.k                      # per default use stored k \n",
    "        if(X==None): X=self.X                      # per default use stored X\n",
    "        return np.argsort([np.linalg.norm(x-a) for a in X])[0:k]   # analog V1A1\n",
    "\n",
    "    def predict(self,x,k=None):\n",
    "        \"\"\" \n",
    "        Implementation of classification algorithm, should be overwritten in any derived classes\n",
    "        :param x: test data vector\n",
    "        :param k: search k nearest neighbors (default self.k)\n",
    "        :returns prediction: label of most likely class that test vector x belongs to\n",
    "                             if there are two or more classes with maximum probability then one class is chosen randomly\n",
    "        :returns pClassPosteriori: A-Posteriori probabilities, pClassPosteriori[i] is probability that x belongs to class i\n",
    "        :returns idxKNN: indexes of the k nearest neighbors (ordered w.r.t. ascending distance) \n",
    "        \"\"\"\n",
    "        if k==None: k=self.k                       # use default parameter k?\n",
    "        idxKNN = self.getKNearestNeighbors(x,k)    # get indexes of k nearest neighbors of x\n",
    "        labelsKNN=[self.T[i] for i in idxKNN]      # list of classes of k nearest neighbors\n",
    "        pClassPosteriori=[labelsKNN.count(i)/float(k) for i in range(self.C)]    #calculate probabilities\n",
    "        p_max=np.max(pClassPosteriori)            #yields highest class probability\n",
    "        c_max=np.where(pClassPosteriori==p_max)[0]   #yields list of classes with maximum probability\n",
    "        prediction=c_max[randint(0,len(c_max)-1)]   #Randomisierung wenn mehrere Klassen gleiche maximale Wahrscheinlichkeit\n",
    "        '''\n",
    "        for cl in range(self.C):                  #iterieren über Klassen\n",
    "            for i in idxKNN:                      #iterieren über Indexe der k nearest neighbors\n",
    "                if self.T[i]==cl:                 #wenn die Klasse des neighbors, der aktuell untersuchten Klasse entspricht,\n",
    "                    pClassPosteriori[cl]+=1/k     #erhöhe die Wahrscheinlichkeit um 1/k        \n",
    "        prediction=np.argmax(pClassPosteriori)      => hier aber keine randomisierte Auswahl wenn mehrere Klassen mit höchster Wahrscheinlichkeit daher auskommentiert!\n",
    "        '''\n",
    "        return prediction, pClassPosteriori, idxKNN  # return predicted class, a-posteriori-distribution, and indexes of nearest neighbors\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# Fast k-nearest-neighbor classifier based on scipy KD trees\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class FastKNNClassifier(KNNClassifier):\n",
    "    \"\"\"\n",
    "    Fast k-nearest-neighbor classifier based on kd-trees \n",
    "    Inherits from class KNNClassifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=2,k=1):\n",
    "        \"\"\"\n",
    "        Constructor of the KNN-Classifier\n",
    "        :param C: Number of different classes\n",
    "        :param k: Number of nearest neighbors that classification is based on\n",
    "        \"\"\"\n",
    "        KNNClassifier.__init__(self,C,k)     # call to parent class constructor \n",
    "        self.kdtree=None\n",
    "\n",
    "    def fit(self,X,T):\n",
    "        \"\"\"\n",
    "        Train classifier by creating a kd-tree \n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        KNNClassifier.fit(self,X,T)                # call to parent class method (just store X and T)\n",
    "        self.kdtree = scipy.spatial.KDTree(X)   # Do an indexing of the feature vectors by constructing a kd-tree\n",
    "        \n",
    "    def getKNearestNeighbors(self, x, k=None):  # realizes fast K-nearest-neighbor-search of x in data set X\n",
    "        \"\"\"\n",
    "        fast computation of the k nearest neighbors for a query vector x given a data matrix X by using the KD-tree\n",
    "        :param x: the query vector x\n",
    "        :param k: number of nearest-neighbors to be returned\n",
    "        :return idxNN: return list of k line indexes referring to the k nearest neighbors of x in X\n",
    "        \"\"\"\n",
    "        if(k==None): k=self.k                      # do a K-NN search...      \n",
    "        dd,ii = self.kdtree.query(x,k)             # do a K-NN search on the generated KD-Tree using scipy.spacial.KDtree.query()\n",
    "        if k==1:                                   # if k==1 the query function does not return lists, therefore cast necessary\n",
    "            idxNN=[ii]\n",
    "        else:\n",
    "            idxNN = ii                             #Store indexes of k nearest neighbors in list idxNN\n",
    "        return idxNN                               # return indexes of k nearest neighbors\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************\n",
    "# __main___\n",
    "# Module test\n",
    "# *******************************************************\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # (i) Generate dummy data \n",
    "    X = np.array([[1,2,3],[2,3,4],[3,4,5],[4,5,6]]);      # data matrix X: list of data vectors (=database) of dimension D=3\n",
    "    T = np.array([0,1,0,1]);                              # target values aka class labels\n",
    "    x = np.array([2,3,4]);                          # a test data vector\n",
    "    print(\"Data matrix X=\\n\",X)\n",
    "    print(\"Class labels T=\\n\",T)\n",
    "    print(\"Test vector x=\",x)\n",
    "    print(\"Euklidean distances d=\",[np.linalg.norm(x-a) for a in X])                     # REPLACE DUMMY CODE (IF YOU WANT) ...\n",
    "\n",
    "    # (ii) Train simple KNN-Classifier\n",
    "    knnc = KNNClassifier()         # construct kNN Classifier\n",
    "    knnc.fit(X,T)                  # train with given data\n",
    "\n",
    "    # (iii) Classify test vector x\n",
    "    k=1\n",
    "    c,pc,idx_knn=knnc.predict(x,k)\n",
    "    print(\"\\nClassification with the naive KNN-classifier:\")\n",
    "    print(\"Test vector is most likely from class \",c)\n",
    "    print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",pc)\n",
    "    print(\"Indexes of the k=\",k,\" nearest neighbors: idx_knn=\",idx_knn)\n",
    "    \n",
    "    # (iv) Repeat steps (ii) and (iii) for the FastKNNClassifier (based on KD-Trees)\n",
    "    \n",
    "    # (iv)-a Train FKNN-Classifier\n",
    "    fknnc=FastKNNClassifier()\n",
    "    fknnc.fit(X,T)\n",
    "    \n",
    "    # (iv)-b Classify test vector x\n",
    "    fk=3\n",
    "    fc,fpc,idx_fknn=fknnc.predict(x,fk)\n",
    "    print(\"\\nClassification with the fast KNN-classifier:\")\n",
    "    print(\"Test vector is most likely from class \",fc)\n",
    "    print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",fpc)\n",
    "    print(\"Indexes of the k=\",fk,\" nearest neighbors: idx_knn=\",idx_fknn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification with the naive KNN-classifier:\n",
      "Test vector is most likely from class  0\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [1.0, 0.0]\n",
      "Indexes of the k= 1  nearest neighbors: idx_knn= [2]\n"
     ]
    }
   ],
   "source": [
    "# (iii) Classify test vector x\n",
    "k=1\n",
    "c,pc,idx_knn=knnc.predict(x,k)\n",
    "print(\"\\nClassification with the naive KNN-classifier:\")\n",
    "print(\"Test vector is most likely from class \",c)\n",
    "print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",pc)\n",
    "print(\"Indexes of the k=\",k,\" nearest neighbors: idx_knn=\",idx_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification with the naive KNN-classifier:\n",
      "Test vector is most likely from class  1\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [0.5, 0.5]\n",
      "Indexes of the k= 2  nearest neighbors: idx_knn= [2 1]\n"
     ]
    }
   ],
   "source": [
    "# (iii) Classify test vector x\n",
    "k=2\n",
    "c,pc,idx_knn=knnc.predict(x,k)\n",
    "print(\"\\nClassification with the naive KNN-classifier:\")\n",
    "print(\"Test vector is most likely from class \",c)\n",
    "print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",pc)\n",
    "print(\"Indexes of the k=\",k,\" nearest neighbors: idx_knn=\",idx_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification with the naive KNN-classifier:\n",
      "Test vector is most likely from class  1\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [0.3333333333333333, 0.6666666666666666]\n",
      "Indexes of the k= 3  nearest neighbors: idx_knn= [2 1 3]\n"
     ]
    }
   ],
   "source": [
    "# (iii) Classify test vector x\n",
    "k=3\n",
    "c,pc,idx_knn=knnc.predict(x,k)\n",
    "print(\"\\nClassification with the naive KNN-classifier:\")\n",
    "print(\"Test vector is most likely from class \",c)\n",
    "print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",pc)\n",
    "print(\"Indexes of the k=\",k,\" nearest neighbors: idx_knn=\",idx_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antwort**: Man sollte für C=2 Klassen ein ungerades k wählen damit der Fall nicht eintritt, dass beide Klassen genau gleich oft in den k nearest Neighbors vorkommen (siehe Bsp. k=2 oben mit 50/50-Wahrscheinlichkeit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification with the fast KNN-classifier:\n",
      "Test vector is most likely from class  0\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [1.0, 0.0]\n",
      "Indexes of the k= 1  nearest neighbors: idx_knn= [2]\n"
     ]
    }
   ],
   "source": [
    "# (iv)-b Classify test vector x\n",
    "fk=1\n",
    "fc,fpc,idx_fknn=fknnc.predict(x,fk)\n",
    "print(\"\\nClassification with the fast KNN-classifier:\")\n",
    "print(\"Test vector is most likely from class \",fc)\n",
    "print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",fpc)\n",
    "print(\"Indexes of the k=\",fk,\" nearest neighbors: idx_knn=\",idx_fknn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification with the fast KNN-classifier:\n",
      "Test vector is most likely from class  0\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [0.5, 0.5]\n",
      "Indexes of the k= 2  nearest neighbors: idx_knn= [2 1]\n"
     ]
    }
   ],
   "source": [
    "# (iv)-b Classify test vector x\n",
    "fk=2\n",
    "fc,fpc,idx_fknn=fknnc.predict(x,fk)\n",
    "print(\"\\nClassification with the fast KNN-classifier:\")\n",
    "print(\"Test vector is most likely from class \",fc)\n",
    "print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",fpc)\n",
    "print(\"Indexes of the k=\",fk,\" nearest neighbors: idx_knn=\",idx_fknn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification with the fast KNN-classifier:\n",
      "Test vector is most likely from class  1\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [0.3333333333333333, 0.6666666666666666]\n",
      "Indexes of the k= 3  nearest neighbors: idx_knn= [2 1 3]\n"
     ]
    }
   ],
   "source": [
    "# (iv)-b Classify test vector x\n",
    "fk=3\n",
    "fc,fpc,idx_fknn=fknnc.predict(x,fk)\n",
    "print(\"\\nClassification with the fast KNN-classifier:\")\n",
    "print(\"Test vector is most likely from class \",fc)\n",
    "print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",fpc)\n",
    "print(\"Indexes of the k=\",fk,\" nearest neighbors: idx_knn=\",idx_fknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(X)\n",
    "S=3# N=number of data vectors\n",
    "perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "Xp,Tp=[X[i] for i in perm], [T[i] for i in perm]    # ... to get a random partition of the data set\n",
    "idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
