{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regressifier**: Abstrakte Klasse als Gerüst für unterschiedliche Regressions-Algorithmen, **DataScaler**: Klasse zur Standardisierung von Daten-Vektoren (wird für einige Regressions-Algorithmen benötigt), **LSRRegressifier**: Implementierung des linearen least squares Regressions-Verfahrens mit Regularisierung, **KNNRegressifier**: Implementierung einer fast KNN-Regression mit KD-Trees\n",
    "- Die abstrakte Methode **fit** dient der späteren Umsetzung des Lern-Algorithmus basierend auf den Daten-Vektoren X und den zugehörigen Labels T, die abstrakte Methode **predict** dient der späteren Umsetzung der \"Vorhersage\" des Regressions-Modells (Prognose von Zielvektor basierend auf Eingangsvektor x), die Methode **crossvalidate** dient der Implementierung einer S-fachen Cross-Validierung der Datenmenge X und der zugehörigen Labels T.\n",
    "- Bei der **Klassifikations-Crossvalidate** wurden einfach Häufigkeiten gezählt (Klassifikation richtig vs. falsch) und daraus Wahrscheinlichkeiten berechnet, bei der **Regressions-Crossvalidate** wird hingegen geschaut wie weit (Abstand) das Modell daneben je Datenpunkt daneben liegt und auf Basis dieser Distanz (absolut vs. relativ, Dimensionen: Durchschnitt, Standard-Abweichung, max/min-Abweichung) die Güte des Regeressions-Modells bewertet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die Funktion **berechnet Polynome vom Grad deg für eine übergebene Liste x**. Besteht die Liste x aus nur einem Wert berechnet die Funktion also x^0 x^1 ... x^deg. Besteht die **Liste aus mehreren Werten** (z.b. x und y), so berechnet sie alle Kombinationen von x * y, d.h. z.B. x^0 * y^0, x^1 * y^0, ... x^deg * y^0, allerdings **nur bis zum Polynomgrad deg=3 (x und y zusammen)**\n",
    "- phi_polynomial([3],5) liefert [1,3,9,27,81,243]\n",
    "- phi_polynomial([3,5],2) liefert [1,3,5,9,15,25]\n",
    "- phi_polynomial([x1,x2],2) liefert allgemein [x1^0 * x2^0, x1^1 * x2^0, x1^0 * x2^1, x1^2 * x2^0, x1^1 * x2^1, x1^0 * x2^2]\n",
    "- Man braucht die Funktion in der Regression um einen Datenvektor x in einen Merkmalsvektor phi mit polynomiellen Basisfunktionen umzurechnen. Die Merkmalsvektoren braucht man wiederum für die Designmatrix mit deren Hilfe man dann z.B. die optimalen Gewichte mittels Least Squares Regression ermitteln kann.\n",
    "- Erweiterung auf Grad 5 s.u."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LSRRegressifier** = Least Squares Regression mit Regularisierung\n",
    "- **lambda** dient der Gewichtung der Regularisierung, **phi** definiert die Basis-Funktionen der linearen Regression (Standard: lineares Polynom vom Grad 1), mit **flagSTD** können die Daten X,T standardisiert werden auf Mittelwert 0 und Standardabweichung 1, **eps** dient der Definition des maximal akzeptierten Restwertes für numerisch gut konditionierte Probleme = verbleibende Ungenauigkeit beim Lösen der Optimierung\n",
    "- Die Klasse **DataScaler** dient dazu, die Daten zu standardisieren (und nach der Regression wieder zu ent-standardisieren). Die Standardisierung erfolgt in der Methode **scale**, die Ent-Standardisierung in der Methode **unscale**. Die Standardisierung funktioniert wie folgt: Subtrahiere den Mittelwert von jeder Komponente und teile durch die Standartabweichung. Diese Methoden werden dann beim Trainieren (Methode **fit**) und bei der Vorhersage (Methode **predict**) angewendet um numerische Instabilitäten zu vermeiden (wenn STD-flag gesetzt). Wenn man die Daten nicht standardisiert, kann es je nach Problemstellung zu fehlerhaften/unbrauchbaren Ergebnissen kommen. Mit den Variablen **Z** und **maxZ** kann man für das gegebene Regressionsproblem dann prüfen wie fehlerhaft/ungenau die Berechnung ist. Wenn das Problem gut konditioniert ist, sollte die Matrix-Invertierung Z = 0 ergeben und damit auch der maximale Wert maxZ in Z = 0 sein und liegt dann unterhalb des maximal akzeptierten Restwertes. Wenn das Problem weniger gut konditioniert ist, kann der maximale Wert maxZ größer sein als der definierte maximal akzeptierte Restwert (eps) und man kann die Ergebnisse der Regression dann nicht verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die Klasse **KNNRegressifier** basiert zunächst auf der K-nearest-neighbor-Suche (vgl. Versuch 1) und nutzt dann entweder die LeastSquares-Regression auf den gefundenen K-nearest-neighbors oder aber einfach den Durchschnitt der K-nearest-neighbor für die Prädiktion y(x). Die Daten X,T werden zunächst mit der Methode **fit** gespeichert (=trainiert). Anschließend kann man für einen neuen Vektor x mit **predict** den zugehörigen Wert y(x) vorhersagen. Dies erfolgt dann entweder auf Basis des Durchschnitts-t-Wertes der k-nearest-neighbor oder aber auf einer least-squares Regression auf den k-nearest neighbor.\n",
    "- Der Parameter **K** gibt dabei an wieviele nächste Nachbarn berücksichtigt werden sollen, der Parameter **flagKLinReg** gibt an, ob die Prädiktion über eine LeastSquares-Regression auf den K-nearest-neighbors erfolgen soll oder ob einfach der Durchschnitt der K-nearest-neighbor als Prognose-Wert y(x) gewählt wird (flag=0 dann Durchschnitt, flag>0 least squares regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modultest:\n",
    "1. 100 Daten-Punkte (X,T) erzeugen (wahre Funktion y=4+2x zzgl. Noise)\n",
    "2. Basisfunktion definieren (lineares Polynom vom Grad 2, d.h. 1, x, x*x)\n",
    "3. LSR berechnen (Gewichte) und dann auf einen Test-Vektor x=3.1415 anwenden\n",
    "4. Kreuzvalidierung für LSR durchführen\n",
    "5. KNN-Regression berechnen (mit K=5 und der Durchschnittsmethode (s.o.)) und auf Test-Vektor anwenden\n",
    "6. Kreuzvalidierung für KNN-Regression durchführen\n",
    "\n",
    "... Fortsetzung Aufgabe 2e) unten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------\n",
      "Example: 1D-linear regression problem\n",
      "-----------------------------------------\n",
      "X= [[ 0. ]\n",
      " [ 0.5]\n",
      " [ 1. ]\n",
      " [ 1.5]\n",
      " [ 2. ]\n",
      " [ 2.5]\n",
      " [ 3. ]\n",
      " [ 3.5]\n",
      " [ 4. ]\n",
      " [ 4.5]\n",
      " [ 5. ]\n",
      " [ 5.5]\n",
      " [ 6. ]\n",
      " [ 6.5]\n",
      " [ 7. ]\n",
      " [ 7.5]\n",
      " [ 8. ]\n",
      " [ 8.5]\n",
      " [ 9. ]\n",
      " [ 9.5]\n",
      " [10. ]\n",
      " [10.5]\n",
      " [11. ]\n",
      " [11.5]\n",
      " [12. ]\n",
      " [12.5]\n",
      " [13. ]\n",
      " [13.5]\n",
      " [14. ]\n",
      " [14.5]\n",
      " [15. ]\n",
      " [15.5]\n",
      " [16. ]\n",
      " [16.5]\n",
      " [17. ]\n",
      " [17.5]\n",
      " [18. ]\n",
      " [18.5]\n",
      " [19. ]\n",
      " [19.5]\n",
      " [20. ]\n",
      " [20.5]\n",
      " [21. ]\n",
      " [21.5]\n",
      " [22. ]\n",
      " [22.5]\n",
      " [23. ]\n",
      " [23.5]\n",
      " [24. ]\n",
      " [24.5]\n",
      " [25. ]\n",
      " [25.5]\n",
      " [26. ]\n",
      " [26.5]\n",
      " [27. ]\n",
      " [27.5]\n",
      " [28. ]\n",
      " [28.5]\n",
      " [29. ]\n",
      " [29.5]\n",
      " [30. ]\n",
      " [30.5]\n",
      " [31. ]\n",
      " [31.5]\n",
      " [32. ]\n",
      " [32.5]\n",
      " [33. ]\n",
      " [33.5]\n",
      " [34. ]\n",
      " [34.5]\n",
      " [35. ]\n",
      " [35.5]\n",
      " [36. ]\n",
      " [36.5]\n",
      " [37. ]\n",
      " [37.5]\n",
      " [38. ]\n",
      " [38.5]\n",
      " [39. ]\n",
      " [39.5]\n",
      " [40. ]\n",
      " [40.5]\n",
      " [41. ]\n",
      " [41.5]\n",
      " [42. ]\n",
      " [42.5]\n",
      " [43. ]\n",
      " [43.5]\n",
      " [44. ]\n",
      " [44.5]\n",
      " [45. ]\n",
      " [45.5]\n",
      " [46. ]\n",
      " [46.5]\n",
      " [47. ]\n",
      " [47.5]\n",
      " [48. ]\n",
      " [48.5]\n",
      " [49. ]\n",
      " [49.5]]\n",
      "T= [[  1.79849963]\n",
      " [  4.10557112]\n",
      " [  7.01314228]\n",
      " [  6.3166319 ]\n",
      " [  6.60769261]\n",
      " [  8.41470404]\n",
      " [ 11.08290497]\n",
      " [ 12.43987496]\n",
      " [ 13.48562733]\n",
      " [ 12.28751146]\n",
      " [ 15.99039479]\n",
      " [ 14.58284865]\n",
      " [ 15.4544316 ]\n",
      " [ 17.2126356 ]\n",
      " [ 18.37997237]\n",
      " [ 19.0681519 ]\n",
      " [ 18.81957526]\n",
      " [ 19.59551533]\n",
      " [ 21.87482701]\n",
      " [ 22.8750119 ]\n",
      " [ 23.81833838]\n",
      " [ 27.63069558]\n",
      " [ 26.76968477]\n",
      " [ 29.55823749]\n",
      " [ 27.31550811]\n",
      " [ 29.24669949]\n",
      " [ 29.29254928]\n",
      " [ 31.88304366]\n",
      " [ 31.76489362]\n",
      " [ 32.70529763]\n",
      " [ 32.71407166]\n",
      " [ 35.30991978]\n",
      " [ 36.10501967]\n",
      " [ 37.80583087]\n",
      " [ 38.39850043]\n",
      " [ 37.21467315]\n",
      " [ 39.19589657]\n",
      " [ 40.03615584]\n",
      " [ 40.82922323]\n",
      " [ 42.58838301]\n",
      " [ 44.22147265]\n",
      " [ 45.28144888]\n",
      " [ 45.45436099]\n",
      " [ 48.14728739]\n",
      " [ 47.44679991]\n",
      " [ 47.82849645]\n",
      " [ 51.14584714]\n",
      " [ 51.83892628]\n",
      " [ 52.39743212]\n",
      " [ 54.13765874]\n",
      " [ 54.19513241]\n",
      " [ 56.53266908]\n",
      " [ 56.22965516]\n",
      " [ 56.90524313]\n",
      " [ 55.61407666]\n",
      " [ 57.57334103]\n",
      " [ 61.3082768 ]\n",
      " [ 62.24105324]\n",
      " [ 60.67270986]\n",
      " [ 63.19182378]\n",
      " [ 65.22242799]\n",
      " [ 65.03362608]\n",
      " [ 66.52468895]\n",
      " [ 67.41318022]\n",
      " [ 69.12219207]\n",
      " [ 68.30013194]\n",
      " [ 68.8587753 ]\n",
      " [ 70.4060622 ]\n",
      " [ 72.01804012]\n",
      " [ 72.14714026]\n",
      " [ 75.05565714]\n",
      " [ 74.93934278]\n",
      " [ 75.28980594]\n",
      " [ 77.22947186]\n",
      " [ 77.65799772]\n",
      " [ 81.02959219]\n",
      " [ 81.27789039]\n",
      " [ 79.6039508 ]\n",
      " [ 80.69364093]\n",
      " [ 82.98777017]\n",
      " [ 84.24307536]\n",
      " [ 86.54681678]\n",
      " [ 87.49580099]\n",
      " [ 85.76380547]\n",
      " [ 87.23928928]\n",
      " [ 87.78238991]\n",
      " [ 90.12618531]\n",
      " [ 90.58582062]\n",
      " [ 93.04491032]\n",
      " [ 91.77932274]\n",
      " [ 95.58158098]\n",
      " [ 94.55801676]\n",
      " [ 95.18930086]\n",
      " [ 97.02636029]\n",
      " [ 97.04416285]\n",
      " [ 99.9788232 ]\n",
      " [ 99.18764168]\n",
      " [100.07030424]\n",
      " [101.89537332]\n",
      " [103.16577012]]\n",
      "phi(4)= [ 1  4 16]\n",
      "phi([1,2])= [1. 1. 2. 1. 2. 4.]\n",
      "\n",
      "-----------------------------------------\n",
      "Do a Least-Squares-Regression\n",
      "-----------------------------------------\n",
      "lsr.W_LSR= [[ 3.85200622e+00]\n",
      " [ 2.02042856e+00]\n",
      " [-4.47002463e-04]]\n",
      "prediction of x= [3.1415] is y= [10.19477107]\n",
      "LSRRegression cross-validation: absolute errors (E,sd,min,max)= (0.8765485235863467, 0.6302494599678162, 0.001251277556804098, 2.683547759116145)   relative errors (E,sd,min,max)= (0.038623336496949025, 0.11981130202061321, 1.228002328285027e-05, 1.1750429648619194)\n",
      "\n",
      "-----------------------------------------\n",
      "Do a KNN-Regression\n",
      "-----------------------------------------\n",
      "prediction of x= [3.1415] is y= 10.406160783591357\n",
      "KNNRegression cross-validation: absolute errors (E,sd,min,max)= (1.3944373962786525, 1.3469429887856723, 0.03481851665372915, 7.437548625349308)   relative errors (E,sd,min,max)= (0.06957836341715572, 0.31049541098089073, 0.000523392400690923, 3.061997452561442)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# V2A2_Regression.py\n",
    "# Programmgeruest zu Versuch 2, Aufgabe 2\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "from random import randint\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# base class for regressifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Regressifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for regressifiers\n",
    "    Inherit from this class to implement a concrete regression algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self,X,T):        # train/compute regression with lists of feature vectors X and class labels T\n",
    "        \"\"\"\n",
    "        Train regressifier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self,x):      # predict a target vector given the data vector x \n",
    "        \"\"\"\n",
    "        Implementation of the regression algorithm; should be overwritten by any derived class \n",
    "        :param x: test data vector of size D\n",
    "        :returns: predicted target vector\n",
    "        \"\"\"\n",
    "        return None           \n",
    "\n",
    "    def crossvalidate(self,S,X,T,dist=lambda t: np.linalg.norm(t)):  # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Matrix of target vectors; T[n] is target vector of X[n]\n",
    "        :param dist: a fuction dist(t) returning the length of vector t (default=Euklidean)\n",
    "        :returns (E_dist,sd_dist,E_min,E_max) : mean, standard deviation, minimum, and maximum of absolute error \n",
    "        :returns (Erel_dist,sdrel_dist,Erel_min,Erel_max) : mean, standard deviation, minimum, and maximum of relative error \n",
    "        \"\"\"\n",
    "        X,T=np.array(X),np.array(T)                         # ensure array type\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        X1,T1=[X[i] for i in perm], [T[i] for i in perm]    # ... to get random partitions of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        E_dist,E_dist2,E_max,E_min=0,0,-1,-1                # initialize first two moments of (absolute) regression error as well as max/min error \n",
    "        Erel_dist,Erel_dist2,Erel_max,Erel_min=0,0,-1,-1    # initialize first two moments of relative regression error as well as max/min error \n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = np.array([X1[i] for i in idxLearn]), np.array([T1[i] for i in idxLearn]) # learn data \n",
    "            X_test , T_test  = np.array([X1[i] for i in idxTest ]), np.array([T1[i] for i in idxTest ]) # test data \n",
    "            self.fit(X_learn,T_learn)                       # train regressifier\n",
    "            # (ii) test regressifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) regress for i-th test vector\n",
    "                xn_test = X_test[i].T                           # data vector for testing\n",
    "                t_test = self.predict(xn_test)                  # predict target value for given test vector \n",
    "                # (ii.b) check for regression errors\n",
    "                t_true = T_test[i].T                            # true target value\n",
    "                d=dist(t_test-t_true)                           # (Euklidean) distance between t_test and t_true \n",
    "                dttrue=dist(t_true)                             # length of t_true\n",
    "                E_dist  = E_dist+d                              # sum up distances (for first moment)\n",
    "                E_dist2 = E_dist2+d*d                           # sum up squared distances (for second moment)\n",
    "                if(E_max<0)or(d>E_max): E_max=d                 # collect maximal error\n",
    "                if(E_min<0)or(d<E_min): E_min=d                 # collect minimal error\n",
    "                drel=d/dttrue\n",
    "                Erel_dist  = Erel_dist+drel                     # sum up relative distances (for first moment)\n",
    "                Erel_dist2 = Erel_dist2+(drel*drel)             # sum up squared relative distances (for second moment)\n",
    "                if(Erel_max<0)or(drel>Erel_max): Erel_max=drel  # collect maximal relative error\n",
    "                if(Erel_min<0)or(drel<Erel_min): Erel_min=drel  # collect minimal relative error\n",
    "        E_dist      = E_dist/float(N)                           # estimate of first moment (expected error)\n",
    "        E_dist2     = E_dist2/float(N)                          # estimate of second moment (expected squared error)\n",
    "        Var_dist    = E_dist2-E_dist*E_dist                     # variance of error\n",
    "        sd_dist     = np.sqrt(Var_dist)                         # standard deviation of error\n",
    "        Erel_dist   = Erel_dist/float(N)                        # estimate of first moment (expected error)\n",
    "        Erel_dist2  = Erel_dist2/float(N)                       # estimate of second moment (expected squared error)\n",
    "        Varrel_dist = Erel_dist2-Erel_dist*Erel_dist            # variance of error\n",
    "        sdrel_dist  = np.sqrt(Varrel_dist)                      # standard deviation of error\n",
    "        return (E_dist,sd_dist,E_min,E_max), (Erel_dist,sdrel_dist,Erel_min,Erel_max) # return mean, standard deviation, minimum, \n",
    "                                                                # and maximum error (for absolute and relative distances)  \n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------- \n",
    "# DataScaler: scale data to standardize data distribution (for mean=0, standard deviation =1)  \n",
    "# -------------------------------------------------------------------------------------------- \n",
    "class DataScaler: \n",
    "    \"\"\"\n",
    "    Class for standardizing data vectors \n",
    "    Some regression methods require standardizing of data before training to avoid numerical instabilities!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,X):               # X is data matrix, where rows are data vectors\n",
    "        \"\"\"\n",
    "        Constructor: Set parameters (mean, std,...) to standardize data matrix X\n",
    "        :param X: Data matrix of size NxD the standardization parameters (mean, std, ...) should be computed for \n",
    "        :returns: object of class DataScaler\n",
    "        \"\"\"\n",
    "        self.meanX = np.mean(X,0)       # mean values for each feature column\n",
    "        self.stdX  = np.std(X,0)        # standard deviation for each feature column \n",
    "        if isinstance(self.stdX,(list,tuple,np.ndarray)): \n",
    "            self.stdX[self.stdX==0]=1.0 # do not scale data with zero std (that is, constant features)\n",
    "        else:\n",
    "            if(self.stdX==0): self.stdX=1.0   # in case stdX is a scalar\n",
    "        self.stdXinv = 1.0/self.stdX    # inverse standard deviation\n",
    "\n",
    "\n",
    "    def scale(self,x):                  # scales data vector x to mean=0 and std=1\n",
    "        \"\"\"\n",
    "        scale data vector (or data matrix) x to mean=0 and s.d.=1 \n",
    "        :param x: data vector or data matrix  \n",
    "        :returns: scaled (standardized) data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x-self.meanX,self.stdXinv)\n",
    "\n",
    "    def unscale(self,x):                # unscale data vector x to original distribution\n",
    "        \"\"\"\n",
    "        unscale data vector (or data matrix) x to original data ranges  \n",
    "        :param x: standardized data vector or data matrix  \n",
    "        :returns: unscaled data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x,self.stdX)+self.meanX\n",
    "\n",
    "    def printState(self):\n",
    "        \"\"\"\n",
    "        print standardization parameters (mean value, standard deviation (std), and inverse of std)  \n",
    "        \"\"\"\n",
    "        print(\"mean=\",self.meanX, \" std=\",self.stdX, \" std_inv=\",self.stdXinv)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# function to compute polynomial basis functions \n",
    "# ----------------------------------------------------------------------------------------- \n",
    "def phi_polynomial(x,deg=1):           # x should be list or np.array or 1xD matrix; returns an 1xM matrix \n",
    "    \"\"\"\n",
    "    polynomial basis function vector; may be used to transform a data vector x into a feature vector phi(x) having polynomial basis function components\n",
    "    :param x: data vector to be transformed into a feature vector\n",
    "    :param deg: degree of polynomial\n",
    "    :returns phi: feature vector \n",
    "    Example: phi_polynomial(x,3) returns for one-dimensional x the vector [1, x, x*x, x*x*x]\n",
    "    \"\"\"\n",
    "    x=np.array(np.mat(x))[0]           # ensure that x is a 1D array (first row of x)\n",
    "    D=len(x)\n",
    "    assert (D==1) or ((D>1) and (deg<=5)), \"phi_polynomial(x,deg) not implemented for D=\"+str(D)+\" and deg=\"+str(deg)    # MODIFY CODE HERE FOR deg>3 !!!!\n",
    "    if(D==1):\n",
    "        phi = np.array([x[0]**i for i in range(deg+1)])\n",
    "    else:\n",
    "        phi = np.array([])\n",
    "        if(deg>=0):\n",
    "            phi = np.concatenate((phi,[1]))      # include degree 0 terms\n",
    "            if(deg>=1): \n",
    "                phi = np.concatenate((phi,x))    # includes degree 1 terms\n",
    "                if(deg>=2):\n",
    "                    for i in range(D):\n",
    "                        phi = np.concatenate(( phi, [x[i]*x[j] for j in range(i+1)] ))    # include degree 2 terms\n",
    "                    if(deg>=3):\n",
    "                        for i in range(D):\n",
    "                            for j in range(i+1):\n",
    "                                phi = np.concatenate(( phi, [x[i]*x[j]*x[k] for k in range(j+1)] ))   # include degree 3 terms\n",
    "                        if(deg>=4):\n",
    "                            for i in range(D):\n",
    "                                for j in range(i+1):\n",
    "                                    for k in range(j+1):\n",
    "                                        phi = np.concatenate((phi, [x[i]*x[j]*x[k]*x[l] for l in range(k+1)])) #include degree 4 terms\n",
    "                            if(deg>=5):\n",
    "                                for i in range(D):\n",
    "                                    for j in range(i+1):\n",
    "                                        for k in range(j+1):\n",
    "                                            for l in range(k+1):\n",
    "                                                phi = np.concatenate((phi, [x[i]*x[j]*x[k]*x[l]*x[m] for m in range(l+1)])) #include degree 5 terms\n",
    "\n",
    "    return phi.T  # return basis function vector (=feature vector corresponding to data vector x)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# Least Squares (ML) linear regression with sum of squares Regularization,\n",
    "# -----------------------------------------------------------------------------------------\n",
    "class LSRRegressifier(Regressifier):\n",
    "    \"\"\"\n",
    "    Class for Least Squares (or Maximum Likelihood) Linear Regressifier with sum of squares regularization \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda=0,phi=lambda x: phi_polynomial(x,1),flagSTD=0,eps=1e-6):\n",
    "        \"\"\"\n",
    "        Constructor of class LSRegressifier\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :param eps: maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.lmbda=lmbda       # set regression parameter (default 0)\n",
    "        self.phi=phi           # set basis functions used for linear regression (default: degree 1 polynomials)\n",
    "        self.flagSTD=flagSTD;  # if flag >0 then data will be standardized, i.e., scaled for mean 0 and s.d. 1\n",
    "        self.eps=eps;          # maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "\n",
    "\n",
    "    def fit(self,X,T,lmbda=None,phi=None,flagSTD=None): # train/compute LS regression with data matrix X and target value matrix T\n",
    "        \"\"\"\n",
    "        Train regressifier (see lecture manuscript, theorem 3.11, p33) \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: flagOK: if >0 then all is ok, otherwise matrix inversion was bad conditioned (and results should not be trusted!!!) \n",
    "        \"\"\"\n",
    "        # (i) set parameters\n",
    "        if lmbda==None: lmbda=self.lmbda       # reset regularization coefficient?\n",
    "        if phi==None: phi=self.phi             # reset basis functions?\n",
    "        if flagSTD==None: flagSTD=self.flagSTD # standardize data vectors?\n",
    "        # (ii) scale data for mean=0 and s.d.=0 ?\n",
    "        if flagSTD>0:                          # if yes, then...\n",
    "            self.datascalerX=DataScaler(X)     # create datascaler for data matrix X\n",
    "            self.datascalerT=DataScaler(T)     # create datascaler for target matrix T\n",
    "            X=self.datascalerX.scale(X)        # scale all features (=columns) of data matrix X to mean=0 and s.d.=1\n",
    "            T=self.datascalerT.scale(T)        # ditto for target matrix T\n",
    "        # (iii) compute weight matrix and check numerical condition\n",
    "        flagOK,maxZ=1,0;                       # if <1 then matrix inversion is numerically infeasible\n",
    "        try:\n",
    "            self.N,self.D = X.shape            # data matrix X has size N x D (N is number of data vectors, D is dimension of a vector)\n",
    "            self.M = self.phi(self.D*[0]).size # get number of basis functions  \n",
    "            #self.K = T.shape[1]                # DELTE dummy code (just required for dummy code in predict(.): number of output dimensions\n",
    "            PHI = np.array([self.phi(X[i]).T for i in range(self.N)])            # REPLACE dummy code: compute design matrix\n",
    "            PHIT_PHI_lmbdaI = np.dot(PHI.T,PHI)+lmbda*np.eye(self.M)             # REPLACE dummy code: compute PHI_T*PHI+lambda*I\n",
    "            PHIT_PHI_lmbdaI_inv = np.linalg.inv(PHIT_PHI_lmbdaI)                 # REPLACE dummy code: compute inverse matrix (may be bad conditioned and fail)\n",
    "            self.W_LSR = np.dot(np.dot(PHIT_PHI_lmbdaI_inv,PHI.T),T)             # REPLACE dummy code: compute regularized least squares weights \n",
    "            # (iv) check numerical condition\n",
    "            Z=np.dot(PHIT_PHI_lmbdaI,PHIT_PHI_lmbdaI_inv)-np.eye(self.M)       # REPLACE dummy code: Compute Z:=PHIT_PHI_lmbdaI*PHIT_PHI_lmbdaI_inv-I which should become the zero matrix if good conditioned!\n",
    "            maxZ = np.max(Z)                                                   # REPLACE dummy code: Compute maximum (absolute) componente of matrix Z (should be <eps for good conditioned problem)\n",
    "            assert maxZ<=self.eps              # maxZ should be <eps for good conditioned problems (otherwise the result cannot be trusted!!!)\n",
    "        except: \n",
    "            flagOK=0;\n",
    "            print(\"EXCEPTION DUE TO BAD CONDITION:flagOK=\", flagOK, \" maxZ=\", maxZ)\n",
    "            raise\n",
    "        return flagOK \n",
    "\n",
    "    def predict(self,x,flagSTD=None):      # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: predicted target vector y of size K\n",
    "        \"\"\"\n",
    "        if flagSTD==None: flagSTD=self.flagSTD      # standardization?\n",
    "        if flagSTD>0: x=self.datascalerX.scale(x)   # if yes, then scale x before computing the prediction!\n",
    "        phi_of_x = self.phi(x)                      # compute feature vector phi_of_x for data vector x\n",
    "        y=np.dot(self.W_LSR.T,phi_of_x)\n",
    "        #y=np.zeros((1,self.K)).T                    # REPLACE dummy code:  compute prediction y for data vector x \n",
    "        if flagSTD>0: y=self.datascalerT.unscale(y) # scale prediction back to original range?\n",
    "        return y                  # return prediction y for data vector x\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# KNN regression \n",
    "# -----------------------------------------------------------------------------------------\n",
    "class KNNRegressifier(Regressifier): \n",
    "    \"\"\"\n",
    "    Class for fast K-Nearest-Neighbor-Regression using KD-trees \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,K,flagKLinReg=0):\n",
    "        \"\"\"\n",
    "        Constructor of class KNNRegressifier\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.K = K                                 # K is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]                     # initially no data is stored\n",
    "        self.flagKLinReg=flagKLinReg               # if flag is set then do a linear regression of the KNN (otherwise just return mean T of the KNN)\n",
    "\n",
    "    def fit(self,X,T): # train/compute regression with lists of data vectors X and target values T\n",
    "        \"\"\"\n",
    "        Train regressifier by stroing X and T and by creating a KD-Tree based on X   \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.X, self.T = np.array(X),np.array(T)   # just store feature vectors X and corresponding class labels T\n",
    "        self.N, self.D = self.X.shape              # store data number N and dimension D\n",
    "        self.kdtree = scipy.spatial.KDTree(self.X) # do an indexing of the feature vectors\n",
    "\n",
    "    def predict(self,x,K=None,flagKLinReg=None):   # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: predicted target vector of size K\n",
    "        \"\"\"\n",
    "        if(K==None): K=self.K                      # do a K-NN search...\n",
    "        if(flagKLinReg==None): flagKLinReg=self.flagKLinReg # if flag >0 then do a regression on the K Nearest Neighbors to get the prediction\n",
    "        nn = self.kdtree.query(x,K)                # get indexes of K nearest neighbors of x\n",
    "        if K==1: idxNN=[nn[1]]                     # cast nearest neighbor indexes nn as a list idxNN\n",
    "        else: idxNN=nn[1]\n",
    "        t_out=0\n",
    "        if(self.flagKLinReg==0):\n",
    "            # just take mean value of KNNs\n",
    "            t_out=np.mean([self.T[i] for i in idxNN])\n",
    "        else:\n",
    "            # do a linear regression of the KNNs\n",
    "            lsr=LSRegressifier(lmbda=0.0001,phi=lambda x:phi_polynomial(x,1),flagSTD=1)\n",
    "            lsr.fit(self.X[idxNN],self.T[idxNN])\n",
    "            t_out=lsr.predict(x)\n",
    "        return t_out\n",
    "\n",
    "\n",
    "# *******************************************************\n",
    "# __main___\n",
    "# Module test\n",
    "# *******************************************************\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Example: 1D-linear regression problem\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    # (i) generate data\n",
    "    N=100\n",
    "    w0,w1=4,2                 # parameters of line\n",
    "    X=np.zeros((N,1))         # x data: allocate Nx1 matrix as numpy ndarray\n",
    "    X[:,0]=np.arange(0,50.0,50.0/N)  # equidistant sampling of the interval [0,50)\n",
    "    T=np.zeros((N,1))         # target values: allocate Nx1 matrix as numpy ndarray\n",
    "    sd_noise = 1.0            # noise power (=standard deviation)\n",
    "    T=T+w1*X+w0 + np.random.normal(0,sd_noise,T.shape)  # generate noisy target values on line y=w0+w1*x\n",
    "    par_lambda = 0            # regularization parameter\n",
    "    print(\"X=\",X)\n",
    "    print(\"T=\",T)\n",
    "\n",
    "    # (ii) define basis functions (phi should return list of basis functions; x should be a list)\n",
    "    deg=2;                               # degree of polynomial\n",
    "    phi=lambda x: phi_polynomial(x,2)    # define phi by polynomial basis-functions up to degree deg \n",
    "    print(\"phi(4)=\", phi([4]))           # print basis function vector [1, x, x*x ...] for x=4\n",
    "    print(\"phi([1,2])=\", phi([1,2]))     # print basis function vector for two-dim. inputs (yields many output components) \n",
    "\n",
    "    # (iii) compute LSR regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a Least-Squares-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    lmbda=0;\n",
    "    lsr = LSRRegressifier(lmbda,phi)\n",
    "    lsr.fit(X,T)\n",
    "    print(\"lsr.W_LSR=\",lsr.W_LSR)        # weight vector (should be approximately [w0,w1]=[4,2])\n",
    "    x=np.array([3.1415]).T\n",
    "    print(\"prediction of x=\",x,\"is y=\",lsr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    S=3\n",
    "    err_abs,err_rel = lsr.crossvalidate(S,X,T)\n",
    "    print(\"LSRRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n",
    "\n",
    "    # (iv) compute KNN-regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a KNN-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    K=5;\n",
    "    knnr = KNNRegressifier(K)\n",
    "    knnr.fit(X,T)\n",
    "    print(\"prediction of x=\",x,\"is y=\",knnr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    err_abs,err_rel = knnr.crossvalidate(S,X,T)\n",
    "    print(\"KNNRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gewichte LSR:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Funktion: y = 4,0874 + 2,0105 * x - 0,0002 * x^2 \n",
    "- Für N->unendlich sollte sich dann idealerweise die \"wahre\" Funktion y = 4 + 2 * x ergeben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die 3-fache Kreuzvalidierung liefert den mittleren Erwartungswert, Standardabweichung, minimalen und maximalen Wert des Fehlers des Regressionsmodell (sowohl in absoluten Werten als auch in relativen Werten). In der vorgegebenen Konfiguration des Modultests, erzielt die LSR-Regression gegenüber der KNN-Regression bessere Ergebnisse (geringere Abweichung). Dies ist auch der Fall wenn die Parameter lambda und K verändert/optimiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------\n",
      "Example: 1D-linear regression problem\n",
      "-----------------------------------------\n",
      "X= [[ 0. ]\n",
      " [ 0.5]\n",
      " [ 1. ]\n",
      " [ 1.5]\n",
      " [ 2. ]\n",
      " [ 2.5]\n",
      " [ 3. ]\n",
      " [ 3.5]\n",
      " [ 4. ]\n",
      " [ 4.5]\n",
      " [ 5. ]\n",
      " [ 5.5]\n",
      " [ 6. ]\n",
      " [ 6.5]\n",
      " [ 7. ]\n",
      " [ 7.5]\n",
      " [ 8. ]\n",
      " [ 8.5]\n",
      " [ 9. ]\n",
      " [ 9.5]\n",
      " [10. ]\n",
      " [10.5]\n",
      " [11. ]\n",
      " [11.5]\n",
      " [12. ]\n",
      " [12.5]\n",
      " [13. ]\n",
      " [13.5]\n",
      " [14. ]\n",
      " [14.5]\n",
      " [15. ]\n",
      " [15.5]\n",
      " [16. ]\n",
      " [16.5]\n",
      " [17. ]\n",
      " [17.5]\n",
      " [18. ]\n",
      " [18.5]\n",
      " [19. ]\n",
      " [19.5]\n",
      " [20. ]\n",
      " [20.5]\n",
      " [21. ]\n",
      " [21.5]\n",
      " [22. ]\n",
      " [22.5]\n",
      " [23. ]\n",
      " [23.5]\n",
      " [24. ]\n",
      " [24.5]\n",
      " [25. ]\n",
      " [25.5]\n",
      " [26. ]\n",
      " [26.5]\n",
      " [27. ]\n",
      " [27.5]\n",
      " [28. ]\n",
      " [28.5]\n",
      " [29. ]\n",
      " [29.5]\n",
      " [30. ]\n",
      " [30.5]\n",
      " [31. ]\n",
      " [31.5]\n",
      " [32. ]\n",
      " [32.5]\n",
      " [33. ]\n",
      " [33.5]\n",
      " [34. ]\n",
      " [34.5]\n",
      " [35. ]\n",
      " [35.5]\n",
      " [36. ]\n",
      " [36.5]\n",
      " [37. ]\n",
      " [37.5]\n",
      " [38. ]\n",
      " [38.5]\n",
      " [39. ]\n",
      " [39.5]\n",
      " [40. ]\n",
      " [40.5]\n",
      " [41. ]\n",
      " [41.5]\n",
      " [42. ]\n",
      " [42.5]\n",
      " [43. ]\n",
      " [43.5]\n",
      " [44. ]\n",
      " [44.5]\n",
      " [45. ]\n",
      " [45.5]\n",
      " [46. ]\n",
      " [46.5]\n",
      " [47. ]\n",
      " [47.5]\n",
      " [48. ]\n",
      " [48.5]\n",
      " [49. ]\n",
      " [49.5]]\n",
      "T= [[  2.57830908]\n",
      " [  4.70280046]\n",
      " [  5.56844613]\n",
      " [  7.93888344]\n",
      " [  9.39159387]\n",
      " [  9.00561838]\n",
      " [  9.97266805]\n",
      " [ 11.10373317]\n",
      " [ 11.40335555]\n",
      " [ 12.57137615]\n",
      " [ 13.48624958]\n",
      " [ 13.9392578 ]\n",
      " [ 16.19715075]\n",
      " [ 17.53672213]\n",
      " [ 17.79764707]\n",
      " [ 19.46959667]\n",
      " [ 20.25740873]\n",
      " [ 21.62578673]\n",
      " [ 22.664966  ]\n",
      " [ 23.61520975]\n",
      " [ 24.80591475]\n",
      " [ 23.85916133]\n",
      " [ 25.45637753]\n",
      " [ 29.06291946]\n",
      " [ 28.06936464]\n",
      " [ 29.02085872]\n",
      " [ 27.99072065]\n",
      " [ 30.8009625 ]\n",
      " [ 32.06139062]\n",
      " [ 33.49648221]\n",
      " [ 34.1711121 ]\n",
      " [ 34.05390272]\n",
      " [ 37.2152199 ]\n",
      " [ 36.41568876]\n",
      " [ 37.69744856]\n",
      " [ 38.30875828]\n",
      " [ 39.68392818]\n",
      " [ 40.58398581]\n",
      " [ 42.85673854]\n",
      " [ 43.16406839]\n",
      " [ 44.32261867]\n",
      " [ 44.03081237]\n",
      " [ 47.59261785]\n",
      " [ 47.54256581]\n",
      " [ 47.38243625]\n",
      " [ 49.43375888]\n",
      " [ 50.84038011]\n",
      " [ 50.5076095 ]\n",
      " [ 51.65169993]\n",
      " [ 52.33156851]\n",
      " [ 52.99679283]\n",
      " [ 52.79064876]\n",
      " [ 54.88155044]\n",
      " [ 58.50869488]\n",
      " [ 56.01912761]\n",
      " [ 59.68732703]\n",
      " [ 59.42879889]\n",
      " [ 59.73379651]\n",
      " [ 64.14577258]\n",
      " [ 61.45637445]\n",
      " [ 64.30082918]\n",
      " [ 65.0358209 ]\n",
      " [ 66.44171071]\n",
      " [ 67.38898959]\n",
      " [ 68.98401817]\n",
      " [ 70.62221633]\n",
      " [ 69.62195365]\n",
      " [ 71.86873876]\n",
      " [ 72.82285505]\n",
      " [ 73.05352167]\n",
      " [ 74.87345644]\n",
      " [ 76.26613349]\n",
      " [ 74.94098382]\n",
      " [ 78.42029871]\n",
      " [ 79.00077683]\n",
      " [ 80.30411881]\n",
      " [ 79.45968009]\n",
      " [ 81.45496218]\n",
      " [ 80.64295619]\n",
      " [ 82.46351795]\n",
      " [ 83.96312726]\n",
      " [ 84.15322634]\n",
      " [ 85.5808412 ]\n",
      " [ 85.51677416]\n",
      " [ 87.38956997]\n",
      " [ 87.54875566]\n",
      " [ 88.24362951]\n",
      " [ 91.3837244 ]\n",
      " [ 91.68348409]\n",
      " [ 94.8419477 ]\n",
      " [ 94.0009854 ]\n",
      " [ 93.64678108]\n",
      " [ 96.27407204]\n",
      " [ 96.85789392]\n",
      " [ 98.47410535]\n",
      " [ 98.57861052]\n",
      " [ 99.15689653]\n",
      " [102.62012798]\n",
      " [100.87170881]\n",
      " [102.26924514]]\n",
      "phi(4)= [ 1  4 16]\n",
      "phi([1,2])= [1. 1. 2. 1. 2. 4.]\n",
      "\n",
      "-----------------------------------------\n",
      "Do a Least-Squares-Regression\n",
      "-----------------------------------------\n",
      "lsr.W_LSR= [[ 3.85877229e+00]\n",
      " [ 2.01723104e+00]\n",
      " [-4.02011887e-04]]\n",
      "prediction of x= [3.1415] is y= [10.19193614]\n",
      "LSRRegression cross-validation: absolute errors (E,sd,min,max)= (0.7876615319298023, 0.568904089743705, 0.013482525144080881, 2.3390428711321647)   relative errors (E,sd,min,max)= (0.0254140654922651, 0.05167159054159879, 0.00025612143921531895, 0.47594726716861724)\n",
      "\n",
      "-----------------------------------------\n",
      "Do a KNN-Regression\n",
      "-----------------------------------------\n",
      "prediction of x= [3.1415] is y= 10.438118210110193\n",
      "KNNRegression cross-validation: absolute errors (E,sd,min,max)= (1.8518000687776486, 1.74572479554561, 0.0029617939848236574, 8.626714717647532)   relative errors (E,sd,min,max)= (0.10309950469100994, 0.3811618927100681, 3.527493653012611e-05, 3.3458807496442806)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----------------------------------------\")\n",
    "print(\"Example: 1D-linear regression problem\")\n",
    "print(\"-----------------------------------------\")\n",
    "# (i) generate data\n",
    "N=100\n",
    "w0,w1=4,2                 # parameters of line\n",
    "X=np.zeros((N,1))         # x data: allocate Nx1 matrix as numpy ndarray\n",
    "X[:,0]=np.arange(0,50.0,50.0/N)  # equidistant sampling of the interval [0,50)\n",
    "T=np.zeros((N,1))         # target values: allocate Nx1 matrix as numpy ndarray\n",
    "sd_noise = 1.0            # noise power (=standard deviation)\n",
    "T=T+w1*X+w0 + np.random.normal(0,sd_noise,T.shape)  # generate noisy target values on line y=w0+w1*x\n",
    "par_lambda = 0            # regularization parameter\n",
    "print(\"X=\",X)\n",
    "print(\"T=\",T)\n",
    "\n",
    "# (ii) define basis functions (phi should return list of basis functions; x should be a list)\n",
    "deg=2;                               # degree of polynomial\n",
    "phi=lambda x: phi_polynomial(x,2)    # define phi by polynomial basis-functions up to degree deg \n",
    "print(\"phi(4)=\", phi([4]))           # print basis function vector [1, x, x*x ...] for x=4\n",
    "print(\"phi([1,2])=\", phi([1,2]))     # print basis function vector for two-dim. inputs (yields many output components) \n",
    "\n",
    "# (iii) compute LSR regression\n",
    "print(\"\\n-----------------------------------------\")\n",
    "print(\"Do a Least-Squares-Regression\")\n",
    "print(\"-----------------------------------------\")\n",
    "lmbda=0.1;\n",
    "lsr = LSRRegressifier(lmbda,phi)\n",
    "lsr.fit(X,T)\n",
    "print(\"lsr.W_LSR=\",lsr.W_LSR)        # weight vector (should be approximately [w0,w1]=[4,2])\n",
    "x=np.array([3.1415]).T\n",
    "print(\"prediction of x=\",x,\"is y=\",lsr.predict(x))\n",
    "\n",
    "# do S-fold crossvalidation\n",
    "S=3\n",
    "err_abs,err_rel = lsr.crossvalidate(S,X,T)\n",
    "print(\"LSRRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n",
    "\n",
    "# (iv) compute KNN-regression\n",
    "print(\"\\n-----------------------------------------\")\n",
    "print(\"Do a KNN-Regression\")\n",
    "print(\"-----------------------------------------\")\n",
    "K=10;\n",
    "knnr = KNNRegressifier(K)\n",
    "knnr.fit(X,T)\n",
    "print(\"prediction of x=\",x,\"is y=\",knnr.predict(x))\n",
    "\n",
    "# do S-fold crossvalidation\n",
    "err_abs,err_rel = knnr.crossvalidate(S,X,T)\n",
    "print(\"KNNRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
